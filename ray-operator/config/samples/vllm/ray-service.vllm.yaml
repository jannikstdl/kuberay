apiVersion: ray.io/v1
kind: RayService
metadata:
  name: ray-service
  namespace: ray
spec:
  serveConfigV2: |
    applications:
    - name: llm
      route_prefix: /
      import_path: ray-operator.config.samples.vllm.serve:model
      deployments:
      - name: VLLMDeployment
        num_replicas: 1
        ray_actor_options:
          num_cpus: 20
      runtime_env:
        working_dir: "https://github.com/jannikstdl/kuberay/archive/master.zip"
        pip: ["vllm==0.5.4"]
        env_vars:
          MODEL_ID: "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
          SERVED_MODEL_NAME:"ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
          TENSOR_PARALLELISM: "1"
          PIPELINE_PARALLELISM: "2"
          GPU_MEMORY_UTILIZATION: "0.90"
          MAX_MODEL_LEN: "8192"
          DOWNLOAD_DIR: "/tmp/huggingface"
  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
      template:
        spec:
          securityContext:
            fsGroup: 1000
            runAsUser: 1000
            runAsGroup: 1000
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: gpu
                        operator: NotIn
                        values:
                          - "true"
          volumes:
            - name: model-storage-head
              persistentVolumeClaim:
                claimName: model-pvc-head
            - name: logs-head
              persistentVolumeClaim:
                claimName: ray-logs-head
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.33.0.914af0-py311
              imagePullPolicy: IfNotPresent
              # Ports belong at top-level inside the container spec:
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              # resources: GPU, CPU, Memory etc. (Ports must NOT be inside resources)
              resources:
                limits:
                  cpu: "2"
                  memory: "4Gi"
              volumeMounts:
                - name: model-storage-head
                  mountPath: /tmp/huggingface
                  readOnly: false
                - name: logs-head
                  mountPath: /tmp/ray
    workerGroupSpecs:
      - groupName: gpu-group-1
        replicas: 1
        minReplicas: 0
        maxReplicas: 1
        rayStartParams: {}
        template:
          spec:
            securityContext:
              fsGroup: 1000
              runAsUser: 1000
              runAsGroup: 1000
            nodeSelector:
              gpu-node: worker1
            volumes:
              - name: model-storage-worker1
                persistentVolumeClaim:
                  claimName: model-pvc-worker1
              - name: logs-worker1
                persistentVolumeClaim:
                  claimName: ray-logs-worker1
            containers:
              - name: llm
                image: rayproject/ray-ml:2.33.0.914af0-py311
                imagePullPolicy: IfNotPresent
                resources:
                  limits:
                    nvidia.com/gpu: "1"
                  requests:
                    nvidia.com/gpu: "1"
                volumeMounts:
                  - name: model-storage-worker1
                    mountPath: /tmp/huggingface
                    readOnly: false
                  - name: logs-worker1
                    mountPath: /tmp/ray
      - groupName: gpu-group-2
        replicas: 1
        minReplicas: 0
        maxReplicas: 1
        rayStartParams: {}
        template:
          spec:
            securityContext:
              fsGroup: 1000
              runAsUser: 1000
              runAsGroup: 1000
            nodeSelector:
              gpu-node: worker2
            volumes:
              - name: model-storage-worker2
                persistentVolumeClaim:
                  claimName: model-pvc-worker2
              - name: logs-worker2
                persistentVolumeClaim:
                  claimName: ray-logs-worker2
            containers:
              - name: llm
                image: rayproject/ray-ml:2.33.0.914af0-py311
                imagePullPolicy: IfNotPresent
                resources:
                  limits:
                    nvidia.com/gpu: "1"
                  requests:
                    nvidia.com/gpu: "1"
                volumeMounts:
                  - name: model-storage-worker2
                    mountPath: /tmp/huggingface
                    readOnly: false
                  - name: logs-worker2
                    mountPath: /tmp/ray
